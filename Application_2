from flask import Flask, render_template, request, jsonify
import cv2
import numpy as np
import base64

app = Flask(__name__)

# Function to resize the image
def resize_image(image, width=None, height=None):
    if width is None and height is None:
        return image
    elif width is None:
        aspect_ratio = height / float(image.shape[0])
        new_width = int(image.shape[1] * aspect_ratio)
        dim = (new_width, height)
    else:
        aspect_ratio = width / float(image.shape[1])
        new_height = int(image.shape[0] * aspect_ratio)
        dim = (width, new_height)
    resized_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
    return resized_image

# Function to compute the integral image manually
def compute_integral_image(image):
    # Initialize an empty array for the integral image
    integral_image = np.zeros_like(image, dtype=np.float32)

    # Compute the cumulative sum over rows for each column separately
    for col in range(image.shape[1]):
        for row in range(image.shape[0]):
            integral_image[row, col] = image[:row+1, col].sum()

    # Compute the cumulative sum over columns for each row separately
    for row in range(integral_image.shape[0]):
        for col in range(1, integral_image.shape[1]):
            integral_image[row, col] += integral_image[row, col-1]

    return integral_image

# Function to perform image stitching
def image_stitch(image1, image2):
    # Convert images to grayscale
    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)

    # Initialize the feature detector and extractor (e.g., SIFT)
    sift = cv2.SIFT_create()

    # Detect keypoints and compute descriptors for both images
    keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)
    keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)

    # Initialize the feature matcher using brute-force matching
    bf = cv2.BFMatcher()

    # Match the descriptors using brute-force matching
    matches = bf.match(descriptors1, descriptors2)

    # Select the top N matches
    num_matches = 50
    matches = sorted(matches, key=lambda x: x.distance)[:num_matches]

    # Extract matching keypoints
    src_points = np.float32([keypoints1[match.queryIdx].pt for match in matches]).reshape(-1, 1, 2)
    dst_points = np.float32([keypoints2[match.trainIdx].pt for match in matches]).reshape(-1, 1, 2)

    # Estimate the homography matrix
    homography, _ = cv2.findHomography(src_points, dst_points, cv2.RANSAC, 5.0)

    # Warp the first image using the homography
    result = cv2.warpPerspective(image1, homography, (image2.shape[1], image2.shape[0]))

    # Blending the warped image with the second image using alpha blending
    alpha = 0.5  # blending factor
    blended_image = cv2.addWeighted(result, alpha, image2, 1 - alpha, 0)

    # Return the blended image
    return blended_image

@app.route('/')
def index():
    return render_template('index.html', dimensions=None)

@app.route('/measure', methods=['POST'])
def measure_dimensions():

    # Check if 'image' and 'known_dimensions' are in the request
    if 'image' not in request.files or 'known_dimensions' not in request.form:
        return "Missing image or known_dimensions in the request", 400
    
    # Receive image and selected points from frontend
    image = request.files['image']
    known_dimensions = int(request.form['known_dimensions'])  # Assuming known dimensions are sent as a string

    # Load the image
    image_np = np.fromfile(image, np.uint8)
    image = cv2.imdecode(image_np, cv2.IMREAD_COLOR)

    # Define the selected corresponding points (pixel coordinates)
    image_points = np.array([(772, 1861), (2292, 1936), (1543, 1253), (1541, 1254)])  # end to end diameter and some other points

    # Define the corresponding real-world coordinates for each pixel coordinate
    real_world_points = np.array([(0, 0), (known_dimensions, 0), (0, known_dimensions), (known_dimensions, known_dimensions)])

    # Compute the homography matrix
    homography, _ = cv2.findHomography(image_points, real_world_points)

    # Define the image coordinates of the object's boundaries
    image_boundary_points = np.array([(0, 0), (image.shape[1], 0), (image.shape[1], image.shape[0]), (0, image.shape[0])], dtype=np.float32)

    # Project the image boundary points onto the real-world plane
    real_world_boundary_points = cv2.perspectiveTransform(image_boundary_points.reshape(-1, 1, 2), homography)

    # Reshape the real_world_boundary_points array
    real_world_boundary_points = real_world_boundary_points.reshape(4, 2)

    # Convert real_world_boundary_points to a nested list of float values using tolist()
    real_world_boundary_points_list = real_world_boundary_points.tolist()

    # Calculate the distance between corresponding real-world boundary points
    real_world_distance = np.linalg.norm(real_world_boundary_points[0] - real_world_boundary_points[1])

    print("Estimated real-world dimensions of the object:", real_world_distance, "cm")

    # Render a new HTML page with the result
    return render_template('index.html', dimensions=real_world_distance)

@app.route('/integral_image', methods=['POST'])
def compute_integral():
    # Receive image from frontend
    image = request.files['image']
    
    # Read the image using OpenCV
    frame = cv2.imdecode(np.frombuffer(image.read(), np.uint8), cv2.IMREAD_COLOR)
    
    # Convert the RGB image to grayscale
    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Compute integral image
    integral_image = compute_integral_image(gray_image)

    # Resize the integral image to a smaller size
    resized_integral_image = resize_image(integral_image, width=500, height=None)

    # Convert integral image to uint8 for display
    integral_image = cv2.normalize(resized_integral_image, None, 0, 255, cv2.NORM_MINMAX)

    # Convert integral image to uint8 for display
    integral_image_uint8 = integral_image.astype(np.uint8)

    # Encode the integral image to base64 for sending to frontend
    _, buffer = cv2.imencode('.jpg', integral_image_uint8)
    integral_image_base64 = base64.b64encode(buffer).decode('utf-8')

    #return jsonify({'integral_image': integral_image_base64})
    # Render a new HTML page with the result
    return render_template('index.html', result=integral_image_base64)

@app.route('/image_stitch', methods=['POST'])
def perform_image_stitch():
    # Receive two images from frontend
    image1 = request.files['image1']
    image2 = request.files['image2']
    
    # Read the images using OpenCV
    frame1 = cv2.imdecode(np.frombuffer(image1.read(), np.uint8), cv2.IMREAD_COLOR)
    frame2 = cv2.imdecode(np.frombuffer(image2.read(), np.uint8), cv2.IMREAD_COLOR)

    # Perform image stitching
    result = image_stitch(frame1, frame2)

    # Resize the integral image to a smaller size
    resized_stitched_image = resize_image(result, width=500, height=None)

    # Encode the stitched image to base64 for sending to frontend
    _, buffer = cv2.imencode('.jpg', resized_stitched_image)
    stitched_image_base64 = base64.b64encode(buffer).decode('utf-8')

    return render_template('index.html', stitched_image=stitched_image_base64)

if __name__ == '__main__':
    app.run(debug=True)